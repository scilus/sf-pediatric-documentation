---
title: Running sf-pediatric without internet access
description: How to run sf-pediatric with no internet access
---

import { Steps } from '@astrojs/starlight/components';
import CommandOutputs from '../../../components/CommandOutputs.astro';
import { Tabs, TabItem } from '@astrojs/starlight/components';

Some computing nodes do not have access to the internet at runtime. Since the pipeline interacts with the container repository and containers are pulled during execution, you will get errors if the compute nodes do not have access to the internet. Fortunately, containers can be downloaded prior to pipeline execution and fetched locally during runtime. To facilitate usage in this specific use case, we provide a download script that handles all container downloads that will be required at runtime.

### **Downloading the containers**

<Steps>

1. **Make sure you have all prerequisites**

    The script relies on three dependencies that should already be available on your compute cluster:

    :::note[Loading a module on a compute cluster]
    For example, on Alliance Canada's clusters, some software comes bundled into modules that you can easily load without having to install them yourself. 
    In this case, you can load the `nextflow` module by using `module load nextflow`. Please refer to the documentation for your specific compute cluster.
    :::

    <Steps>
    1. **`nextflow >= 24.10.5`**
    
    2. **`apptainer` or `singularity`**

    3. **`jq`**
    </Steps>

2. **Run the download script**

    Once all prerequisites are loaded and available in your `PATH`, you can run the following command (if you are on a login node, it is recommended to use maximum `-d 2` for parallel download):

    :::note[Downloading all containers might take a while]
    Due to the high number of dependencies within `sf-pediatric`, we leverage multiple different containers that can take up a lot of space and time to download. We recommend you set your container output directory in filesystem with sufficient space (e.g., `scratch` filesystem). Once download is complete, you can move the containers into a permanent filesystem (e.g., `projects` filesystem). This also applies for the cache location, we strongly recommend a filesystem with large quotas to avoid any potential issues. 
    :::

    <Tabs>
    <TabItem label="Command">
    ```bash
    curl -fsSL https://github.com/nf-neuro/modules/blob/main/assets/download_pipeline.sh | bash -s -- -p scilus/sf-pediatric -r 0.2.1 -d 1 -o ./containers -c ./singularity_cache
    ```
    </TabItem>
    <TabItem label="Expected Output">
    ```bash
    Validating required tools...
    ✓ All required tools are available

    === Nextflow Pipeline Offline Downloader ===
    Pipeline: scilus/sf-pediatric
    Revision: 0.2.1
    Parallel downloads: 1
    Container directory: ./containers
    Cache directory: ./singularity_cache

    Step 1: Pulling Nextflow pipeline...
    Checking scilus/sf-pediatric ...
    done - revision: 17bb3e73e0 [0.2.1]
    ✓ Pipeline pulled successfully

    Step 2: Inspecting pipeline for container requirements...
    ✓ Pipeline inspection complete
    ✓ Found 18 unique container(s)

    Step 3: Downloading Apptainer/Singularity containers...
    This may take a while depending on container sizes...

    [1/18] Downloading: community.wave.seqera.io/library/pip_templateflow:2f726c524c63271e
    [1/18] ✓ Downloaded: community.wave.seqera.io-library-pip_templateflow-2f726c524c63271e
    [2/18] ✓ Already exists: docker.io-freesurfer-freesurfer-7.4.1
    [3/18] ✓ Already exists: docker.io-freesurfer-synthstrip-1.5
    [4/18] ✓ Already exists: docker.io-gagnonanthony-multiqc-neuroimaging-latest
    [5/18] ✓ Already exists: docker.io-gagnonanthony-neurostatx-0.1.0
    [6/18] ✓ Already exists: docker.io-gagnonanthony-nf-pediatric-atlases-1.1.0
    [7/18] ✓ Already exists: docker.io-gagnonanthony-nf-pediatric-fastsurfer-v2.3.3
    [8/18] ✓ Already exists: docker.io-gagnonanthony-nf-pediatric-freesurfer-8.0.0
    [9/18] ✓ Already exists: docker.io-gagnonanthony-nf-pediatric-mcribs-2.1.0
    [10/18] ✓ Already exists: docker.io-mrtrix3-mrtrix3-3.0.5
    [11/18] ✓ Already exists: docker.io-scilus-scilpy-1.6.0
    [12/18] ✓ Already exists: docker.io-scilus-scilpy-2.2.0_cpu
    [13/18] ✓ Already exists: docker.io-scilus-scilpy-2.2.1_cpu
    [14/18] ✓ Already exists: docker.io-scilus-scilpy-dev
    [15/18] ✓ Already exists: docker.io-scilus-scilus-2.0.2
    [16/18] ✓ Already exists: docker.io-scilus-scilus-2.1.0
    [17/18] ✓ Already exists: docker.io-scilus-scilus-2.2.0
    [18/18] ✓ Already exists: docker.io-scilus-scilus-2.2.1

    === Download Complete ===
    ✓ All containers downloaded successfully
    Container location: /lustre10/scratch/agagnon/test_nfcore_download/containers

    To use these containers offline, set the following environment variables in your shell:

    export NXF_SINGULARITY_CACHEDIR='/lustre10/scratch/agagnon/test_nfcore_download/containers'
    export NXF_APPTAINER_CACHEDIR='/lustre10/scratch/agagnon/test_nfcore_download/containers'
    export SINGULARITY_CACHEDIR='/lustre10/scratch/agagnon/test_nfcore_download/containers'
    export APPTAINER_CACHEDIR='/lustre10/scratch/agagnon/test_nfcore_download/containers'

    You can then run your nextflow pipeline offline!
    Refer to the pipeline documentation for usage details.
    ```
    </TabItem>
    </Tabs>

    :::tip[Customizing your download command]
    The download accepts a few parameters that you can customize. Here is a list with a small description:
    - `-p`  Pipeline name referring to the GitHub organization and repository name (`scilus/sf-pediatric`) (required).
    - `-r`  Pipeline tag or branch to pull from (should be a release version, default: `main`).
    - `-d`  Number of parallel download (default: `4`).
    - `-o`  Container output directory (default: `./containers`).
    - `-c`  Singularity/Apptainer cache directory (default: `./singularity_cache`).

    To modify any of those options, simply replace the current values in the command above with your desired value and run the script!
    ::: 
</Steps>

### **Running sf-pediatric using the SLURM executor**

<Steps>
1. **Set environment variables for pipeline execution**

    Now that all containers are downloaded, we need to tell Nextflow where to look for those containers. This can be done using environment variables. For example, if you run the pipeline in the same directory where you downloaded the containers, you should set these environment variables:

    <Tabs>
    <TabItem label="Command">
    ```bash
    export SINGULARITY_CACHEDIR="./singularity_cache"
    export APPTAINER_CACHEDIR="./singularity_cache"
    export NXF_SINGULARITY_CACHEDIR="./containers"
    export NXF_APPTAINER_CACHEDIR="./containers"
    ```
    </TabItem>
    </Tabs>

    :::note[Adapt the paths according to your folder location]
    In the code snippet above, we set the environment variables based on the default values of the download script. If you changed those values or moved the container folder, you should update those environment variables to point to the correct folder. Otherwise, the nextflow execution won't find the containers and error out.
    :::

2. **Launching `sf-pediatric` using the SLURM scheduler**

    One of the great advantage of `nextflow` is that it integrates natively with executors such as SLURM. This means that it can handle job submission for you, by submitting a single job per `nextflow` process. We currently have a designated profile for that, using `-profile slurm,apptainer`. However, since `nextflow` will handle all job submissions for you, it needs a few information prior to running the pipeline.

    First, since `nextflow` will interact for you with the SLURM scheduler, you can launch the pipeline directly on the login node using a terminal multiplexer (`tmux`/`screen`). The terminal multiplexer will enable the pipeline to keep running even if you disconnect from your server.

    :::tip[Opening, exiting, and reattaching a `tmux` session]
    To open a `tmux` session, simply run `tmux` in the desired location. This will open a new session, in which you will run the pipeline as described below. To exit the session without killing it, use the `CTRL-B + D` shortcut. This will close the `tmux` session, but it will keep running the pipeline for you. To reattach to this session, use `tmux a`. For more information on how to use `tmux`, you can refer to this [cheat sheet](https://tmuxcheatsheet.com) or this [introduction](https://www.redhat.com/en/blog/introduction-tmux-linux).
    :::

    Second, inside your `tmux` session, make sure you set your environment variables for your containers as described in section 3 above. Third, you need to set your allocation account to be used for submitting jobs. This can be done using environment variables:

    <Tabs>
    <TabItem label="Command">
    ```bash
    export SLURM_ACCOUNT=<account_name>     # Replace the <account_name> with the name of the allocation to use.
    export SBATCH_ACCOUNT=$SLURM_ACCOUNT
    export SALLOC_ACCOUNT=$SLURM_ACCOUNT
    ```
    </TabItem>
    </Tabs>

    Finally, you can launch the pipeline (see the [Running the pipeline](/sf-pediatric/guides/usage) section for more details) using a simple call to nextflow:

    <Tabs>
    <TabItem label="Command">
    ```bash
    nextflow run scilus/sf-pediatric -r 0.2.1 \
        --input <input_folder> \           # Change this.
        --outdir <output_folder> \         # Change this.
        -profile ...,slurm,apptainer \     # Add any processing profiles you want to run.
        -resume
    ```
    </TabItem>
    </Tabs>

    Alternatively, you can also add everything in a bash script, ensuring you are loading all the required modules and setting the required environment variables. Here's an example:

    <Tabs>
    <TabItem label="cmd_sfpediatric.sh">
    ```bash
    #/usr/bin/env bash

    # Load the required modules.
    module load nextflow apptainer

    # Some CLI arguments for when you launch the script.
    # Setting input & output folder as cli args.
    input=$1
    output=$2
    account=$3
    container_dir=$4

    # Variables for containers, etc.
    export NXF_APPTAINER_CACHEDIR=$4
    export SLURM_ACCOUNT=$3
    export SBATCH_ACCOUNT=$SLURM_ACCOUNT
    export SALLOC_ACCOUNT=$SLURM_ACCOUNT

    # Call for the pipeline execution.
    nextflow run scilus/sf-pediatric -r 0.2.1 \
        --input $input \
        --outdir $output \
        -profile ...,apptainer,slurm \      # Add any processing profiles you want to run.
        -resume
    ```
    </TabItem>
    </Tabs>

    You can then run this bash script **inside your `tmux` session** using a simple one liner (replace each parameter with the one that fits your file structure):

    <Tabs>
    <TabItem label="Command">
    ```bash
    bash cmd_sfpediatric.sh <path/to/input_data> <path/to/output_data> <slurm_account> <path/to/container_folder>
    ```
    </TabItem>
    </Tabs>

</Steps>

### **Running sf-pediatric within a single SBATCH job**

Some clusters do not allow Nextflow pipelines to run on login nodes (even though the login node only handles job scheduling and orchestration, not the actual data processing). In this case, you should run the pipeline within a single SBATCH job. Here is an example, in which we request a single node with 40 CPUs. 

:::note[No `tmux` is required for single SBATCH jobs]
Since all job management will be done directly on a compute node, we do not need to rely on a multiplexer to keep our job alive. This can also be an advantage of submitting a single SBATCH job if your compute cluster does not allow multiplexers.
:::

<Tabs>
<TabItem label="cmd_sfpediatric_sbatch.sh">
```bash
#!/bin/sh
#SBATCH --mail-user=<email>     # Change this.
#SBATCH --mail-type=ALL

#SBATCH --account=<account>     # Change this.
#SBATCH --nodes=1
#SBATCH --cpus-per-task=40  
#SBATCH --mem=128G              # Please refer to your compute nodes characteristics.
#SBATCH --time=48:00:00         # Might need adjusting the time depending on the number of subjects.

# Load the required modules.
module load nextflow apptainer

# Variables for containers, etc.
export NXF_APPTAINER_CACHEDIR=<path/to/container_folder>    # Change this.

# Call for the pipeline execution.
nextflow run scilus/sf-pediatric -r 0.2.1 \
    --input <path/to/input_folder> \    # Change this.
    --outdir <path/to/output_folder> \  # Change this.
    -profile ...,apptainer,slurm \      # Add any processing profiles you want to run.
    -resume
```
</TabItem>
</Tabs>

You can then submit your job by using the `sbatch` command.

<Tabs>
<TabItem label="Command">
```bash
sbatch cmd_sfpediatric_sbatch.sh
```
</TabItem>
</Tabs> 

### **Downloading TemplateFlow templates**

`sf-pediatric` allows users to select a specific output space in which all subjects will be registered. These templates are fetched using the [TemplateFlow Archive](https://www.templateflow.org/) [(Ciric _et al_, 2022)](https://doi.org/10.1038/s41592-022-01681-2). While downloading is directly handled in `sf-pediatric` when internet is available, for offline use, the selected template (specified using the [`--template`](/sf-pediatric/guides/parameters#output-to-template-options) parameter) needs to be available. You can refer to the [official TemplateFlow documentation](https://www.templateflow.org/usage/archive/) for information on how to download the archive using either `datalad` or the Python client.

:::note
Similar to the containers, `sf-pediatric` needs to be told where to look for those downloaded templates. This can be done using the [`--templateflow_home`](/sf-pediatric/guides/parameters#output-to-template-options) parameters.
:::